MLDS HW4-1 - Gradient Policy 
---

### Describe your Policy Gradient model (1%)

#### Model

| layer  | RGB image from observation                                | Shape                      |
| ------ | --------------------------------------------------------- | -------------------------- |
| perpro | thanks to TA that gave us such excellent preprocess code. | observation -> 80*80 image |
| hidden | Dense(128 , activation='relu')                            | (None,6400) -> (None,128)  |
| output | Dense(6,activation='softmax')                             | (None,128) -> (None,6)     |

#### Details

| Details        | Parameteres                         |
| -------------- | ----------------------------------- |
| Optimizer      | pyTorch's default Adam with lr=1e-4 |
| Discount value | 0.99                                |

### Plot the learning curve to show the performance of your Policy Gradient on Pong (1%)

![pg_learning_curve](pg_learning_curve.png)

### Implement 1 improvement method on page 8
#### Describe your tips for improvement (1%)

* variance reduction - add baseline
* use mean(the lastest 10000 rewards, discount inculded) as our baseline.
* When update parameters, reward will -= baseline.

#### Learning curve (1%)

* learning curve is merged with naive pg learning curve and is showed above.

#### Compare to the vallina policy gradient (1%)

* Vallina policy gradient learns very slow.  We think because the mean is unstable to let model learn well.
* In other words, if in one episode, most of battles are win, the reward is almost positive, it may not learn the things that "procrastination in battles are bad." However, if we add baseline, due to discounted rewards, our model will deem the old steps as "bad actions", and try to find a new efficient way to beat the rival up.
* Next, just as it's name, add baseline will reduce the variance. And, if we successfully reduce the variance, it'll increase the precision of the estimates that can be obtained. Due to more precision, the  sampling steps in our policy gradient can be more stable and robust. Because the original target to sampling is to find the expectations.

>  proof : https://en.wikipedia.org/wiki/Variance_reduction