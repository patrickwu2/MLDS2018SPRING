# MLDS hw 1-2

## 1-2-1

### Summary

-> MNIST, DNN, 8070 param. (784*10 + 10*10 + 10*10 + 10 + 10 + 10)
-> batch: 1000, epoch = 4000
-> optimizer: Adam, lr:0.01, loss function: cross entropy
-> random initialize variable
	1: 0, 2: 127, 3: 7122, 4: 1126, 5: 9487, 6: 2718, 7: 31416, 8: 1618
	seddev=1.2
-> Dimension Reduction: 使用sklearn內建的pca就直接降到2維


### Result

1. whole model跟1-layer的圖很相似

## 1-2-2

